# LiteLLM Proxy Configuration
# Start with: litellm --config config.yaml
# Your agents point at http://localhost:4000 instead of the API directly

model_list:
  # Add your models here. Agents call these by model_name.
  - model_name: claude-sonnet
    litellm_params:
      model: anthropic/claude-sonnet-4-20250514
      api_key: os.environ/ANTHROPIC_API_KEY

  - model_name: claude-opus
    litellm_params:
      model: anthropic/claude-opus-4-20250514
      api_key: os.environ/ANTHROPIC_API_KEY

  - model_name: gemini-flash
    litellm_params:
      model: gemini/gemini-2.0-flash
      api_key: os.environ/GEMINI_API_KEY

litellm_settings:
  # This loads the middleware. LiteLLM imports codegraph_middleware.py
  # and registers proxy_handler as a callback on every request.
  callbacks: ["codegraph_middleware.proxy_handler"]

  # Drop requests that take too long (seconds)
  request_timeout: 120

# Proxy server settings
general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY  # Optional: protect proxy with a key
